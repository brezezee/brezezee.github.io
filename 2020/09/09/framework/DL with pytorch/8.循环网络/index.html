<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="../../../../../../lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="../../../../../../lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="../../../../../../css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="../../../../../../images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="../../../../../../images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="../../../../../../images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="../../../../../../images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="基础结构RNN基础单元：  在RNN基础单元中，隐藏层状态由输入层和上一时刻的状态决定，即隐藏层的计算是$X_tW_{xh} + H_{t-1}W_{hh}$ ，而这个计算可以通过将$X_t$与$H_{t-1}$连结后的矩阵与$W_{xh}$与$W_{hh}$连结后的矩阵相乘得到。 12345678import torchX, W_xh = torch.randn(3, 1), torch.ran">
<meta name="keywords" content="ml,dl,cv,ai">
<meta property="og:type" content="article">
<meta property="og:title" content="brezezee">
<meta property="og:url" content="https://brezezee.github.io/2020/09/09/framework/DL with pytorch/8.循环网络/index.html">
<meta property="og:site_name" content="brezezee">
<meta property="og:description" content="基础结构RNN基础单元：  在RNN基础单元中，隐藏层状态由输入层和上一时刻的状态决定，即隐藏层的计算是$X_tW_{xh} + H_{t-1}W_{hh}$ ，而这个计算可以通过将$X_t$与$H_{t-1}$连结后的矩阵与$W_{xh}$与$W_{hh}$连结后的矩阵相乘得到。 12345678import torchX, W_xh = torch.randn(3, 1), torch.ran">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://brezezee.github.io/2020/09/09/framework/DL%20with%20pytorch/8.循环网络/1594438420154.png">
<meta property="og:image" content="https://brezezee.github.io/2020/09/09/framework/DL%20with%20pytorch/8.循环网络/1594721085621.png">
<meta property="og:image" content="https://brezezee.github.io/2020/09/09/framework/DL%20with%20pytorch/8.循环网络/1594893699477.png">
<meta property="og:image" content="https://brezezee.github.io/2020/09/09/framework/DL%20with%20pytorch/8.循环网络/1595067353701.png">
<meta property="og:updated_time" content="2020-09-23T16:22:10.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="brezezee">
<meta name="twitter:description" content="基础结构RNN基础单元：  在RNN基础单元中，隐藏层状态由输入层和上一时刻的状态决定，即隐藏层的计算是$X_tW_{xh} + H_{t-1}W_{hh}$ ，而这个计算可以通过将$X_t$与$H_{t-1}$连结后的矩阵与$W_{xh}$与$W_{hh}$连结后的矩阵相乘得到。 12345678import torchX, W_xh = torch.randn(3, 1), torch.ran">
<meta name="twitter:image" content="https://brezezee.github.io/2020/09/09/framework/DL%20with%20pytorch/8.循环网络/1594438420154.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://brezezee.github.io/2020/09/09/framework/DL with pytorch/8.循环网络/">





  <title> | brezezee</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">brezezee</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="https://brezezee.github.io" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="https://brezezee.github.io/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="https://brezezee.github.io/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-categories"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="https://brezezee.github.io/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="https://brezezee.github.io/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://brezezee.github.io">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="brezezee">
      <meta itemprop="description" content>
      <meta itemprop="image" content="../../../../../../images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="brezezee">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline"></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-09-09T11:35:58+08:00">
                2020-09-09
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2020-09-24T00:22:10+08:00">
                2020-09-24
              </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/09/09/framework/DL with pytorch/8.循环网络/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  4.4k
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="基础结构"><a href="#基础结构" class="headerlink" title="基础结构"></a>基础结构</h2><p>RNN基础单元：</p>
<p><img src="1594438420154.png" alt="1594438420154"></p>
<p>在RNN基础单元中，隐藏层状态由输入层和上一时刻的状态决定，即隐藏层的计算是$X_tW_{xh} + H_{t-1}W_{hh}$ ，而这个计算可以通过将$X_t$与$H_{t-1}$连结后的矩阵与$W_{xh}$与$W_{hh}$连结后的矩阵相乘得到。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">X, W_xh = torch.randn(<span class="number">3</span>, <span class="number">1</span>), torch.randn(<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">H, W_hh = torch.randn(<span class="number">3</span>, <span class="number">4</span>), torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># 先乘后加</span></span><br><span class="line">torch.matmul(X, W_xh) + torch.matmul(H, W_hh)</span><br><span class="line"><span class="comment"># 先连结后乘</span></span><br><span class="line">torch.matmul(torch.cat((X, H), dim=<span class="number">1</span>), torch.cat((W_xh, W_hh), dim=<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<p>这两种方法得到的结果都是一样的。</p>
<h3 id="梯度计算"><a href="#梯度计算" class="headerlink" title="梯度计算"></a>梯度计算</h3><p>以时间序列为3的计算图为例：</p>
<p><img src="1594721085621.png" alt="1594721085621"></p>
<p>其前向计算公式为：</p>
<script type="math/tex; mode=display">
h_t = W_{hx}x_t + W_{hh}h_{t-1}\\
o_t = W_{qh}h_t\\
L = \frac 1 T \sum_{t=1}^T l(o_t, y_t)</script><p>从L开始倒推梯度：</p>
<script type="math/tex; mode=display">
\frac {\partial L} {\partial o_t} = \frac {\part l(o_t,y_t)} {T\cdot \part o_t}</script><p>输出层参数：(sum是因为共享参数，乘积是链式法则)</p>
<script type="math/tex; mode=display">
\frac {\part L}{\part W_{qh}} = \sum_{t=1}^T \frac {\part L}{\part o_t}\cdot \frac{\part o_t}{\part W_{qh}} = \sum_{t=1}^T \frac{\part L}{\part o_t}\cdot h_t^\top</script><p>在计算隐藏层参数$W_{hh}$ 时，它们通过隐藏层状态$h_t$产生了依赖关系。因此先计算最终时间步T的$h_T$的梯度：</p>
<script type="math/tex; mode=display">
\frac {\part L}{\part h_T} = \frac {\part L}{\part o_T}\cdot \frac {\part o_T}{\part h_T} = W_{qh}^\top \frac {\part L}{\part o_T}</script><p>而对于$t &lt; T$的情况，需要从后往前计算，这样对于每个$h_t$的梯度在计算时，后面依赖它的$h$的梯度都被计算出了。则根据链式求导法则$h_t$的梯度包含$o_t$和之后的$h$ 对$h_t$的偏导。于是这变成了一个关于$h_t$的递推式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac {\part L}{\part h_t} &= \frac {\part L}{\part h_{t+1}}\cdot\frac {\part h_{t+1}}{\part h_t}+\frac {\part L}{\part o_t}\cdot \frac {\part L}{\part h_t}\\
&= W_{hh}^\top \cdot \frac {\part L}{\part h_{t+1}} + W_{qh}^\top \cdot \frac {\part L}{\part o_t}\\
(展开h_{t+1})&=\sum_{i = t}^T(W_{hh}^\top)^{T-i} W_{qh}^\top \frac {\part L}{\part o_{T-i + t}}
\end{aligned}</script><p>由上式中的指数项可见，当$T$与$t$之差较大时，容易出现梯度衰减和爆炸。而$h_t$的梯度也会影响其他参数的梯度，比如$W_{hh}、W_{hx}$ ，它们在隐藏层间共享参数，$h_1, \cdots, h_T$都依赖它们，因此：</p>
<script type="math/tex; mode=display">
\frac {\part L}{\part W_{hx}} = \sum_{t =1}^T \frac {\part L}{\part h_t}\frac {\part h_t}{\part W_{hx}}=\sum_{t=1}^T \frac {\part L}{\part h_t}x_t^\top</script><h3 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'../input/jaychou-lyrics/jaychou_lyrics.txt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    corpus_chars = f.read()    <span class="comment"># string</span></span><br><span class="line">corpus_chars[:<span class="number">40</span>]   <span class="comment"># </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">corpus_chars = corpus_chars.replace(<span class="string">'\n'</span>, <span class="string">' '</span>).replace(<span class="string">'\r'</span>, <span class="string">' '</span>)</span><br><span class="line">corpus_chars = corpus_chars[<span class="number">0</span>:<span class="number">10000</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 建立字符索引</span></span><br><span class="line">idx_to_char = list(set(corpus_chars))  <span class="comment"># 先转为set集合，使得无重复字符，再转为list，方便索引访问</span></span><br><span class="line">char_to_idx = dict([(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)])  <span class="comment"># 映射组成字典</span></span><br><span class="line">vocab_size = len(char_to_idx) </span><br><span class="line">vocab_size <span class="comment"># 1027个汉字</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取文本索引</span></span><br><span class="line">corpus_indices = [char_to_idx[char] <span class="keyword">for</span> char <span class="keyword">in</span> corpus_chars]   <span class="comment"># 用它作为语料库</span></span><br><span class="line">sample = corpus_indices[:<span class="number">20</span>]</span><br><span class="line">print(<span class="string">'chars:'</span>, <span class="string">''</span>.join([idx_to_char[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> sample]))</span><br><span class="line">print(<span class="string">'indices:'</span>, sample)</span><br></pre></td></tr></table></figure>
<h3 id="采样时序数据"><a href="#采样时序数据" class="headerlink" title="采样时序数据"></a>采样时序数据</h3><ul>
<li>随机采样</li>
</ul>
<p>下面的代码每次从数据里随机采样一个小批量。其中批量大小<code>batch_size</code>指每个小批量的样本数，<code>num_steps</code>为每个样本所包含的时间步数（一个时间序列）。 在随机采样中，每个样本是原始序列上任意截取的一段序列。相邻的两个随机小批量在原始序列上的位置不一定相毗邻。因此，我们无法用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态。在训练模型时，每次随机采样前都需要重新初始化隐藏状态。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数包含语料索引，批量大小，每个样本的序列长度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_random</span><span class="params">(corpus_indices, batch_size, num_steps, device=None)</span>:</span></span><br><span class="line">    <span class="comment"># 减1是因为样本y需要在x的基础上向后偏移一位</span></span><br><span class="line">    num_examples = (len(corpus_indices) - <span class="number">1</span>) // num_steps <span class="comment"># 可采样出的样本数量</span></span><br><span class="line">    </span><br><span class="line">    epoch_size = num_examples // batch_size  <span class="comment"># 遍历样本需要执行多少个epoch</span></span><br><span class="line">    </span><br><span class="line">    example_indices = list(range(num_examples))  </span><br><span class="line">    random.shuffle(example_indices)          <span class="comment"># 对样本洗牌</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回从pos开始的长为num_steps的序列，即一个样本的序列长度</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_data</span><span class="params">(pos)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> corpus_indices[pos: pos + num_steps]</span><br><span class="line">    <span class="keyword">if</span> device <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">	</span><br><span class="line">    <span class="comment">## 迭代取样本，需迭代epoch_size次,每次返回一个batch_size大小的X和Y</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        <span class="comment"># 每次读取batch_size个随机样本</span></span><br><span class="line">        i = i * batch_size</span><br><span class="line">        batch_indices = example_indices[i: i + batch_size]</span><br><span class="line">        X = [_data(j * num_steps) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        Y = [_data(j * num_steps + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        <span class="keyword">yield</span> torch.tensor(X, dtype=torch.float32, device=device), torch.tensor(Y, dtype=torch.float32, device=device)</span><br></pre></td></tr></table></figure>
<ul>
<li>相邻采样</li>
</ul>
<p>注意相邻采样是在连续的epoch之间相邻，若一个epoch中有多个batch_size，则不同batch之间的样本不一定连续。因此可将样本按batch_size分割，再依次按epoch顺序取。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_consecutive</span><span class="params">(corpus_indices, batch_size, num_steps, device=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> device <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">    corpus_indices = torch.tensor(corpus_indices, dtype=torch.float32, device=device)</span><br><span class="line">    data_len = len(corpus_indices)</span><br><span class="line">    batch_len = data_len // batch_size  <span class="comment"># 按batch分割</span></span><br><span class="line">    <span class="comment">#在每个批次(batch)中，数据是相邻取的，因此可直接将数据分成几个batch再顺序取(不同batch之间不是相邻的)</span></span><br><span class="line">    indices = corpus_indices[<span class="number">0</span>: batch_size*batch_len].view(batch_size, batch_len) <span class="comment"># 保证对齐</span></span><br><span class="line">    epoch_size = (batch_len - <span class="number">1</span>) // num_steps  <span class="comment"># 样本Y后延一位</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        i = i * num_steps</span><br><span class="line">        X = indices[:, i: i + num_steps]</span><br><span class="line">        Y = indices[:, i + <span class="number">1</span>: i + num_steps + <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter_consecutive(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">6</span>):</span><br><span class="line">    print(<span class="string">'X: '</span>, X, <span class="string">'\nY:'</span>, Y, <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="基础RNN"><a href="#基础RNN" class="headerlink" title="基础RNN"></a>基础RNN</h2><h3 id="手动实现"><a href="#手动实现" class="headerlink" title="手动实现"></a>手动实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"><span class="comment">#(corpus_indices, char_to_idx, idx_to_char, vocab_size) = load_data_jay_lyrics()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###################### 将词按序号转化为词向量  ############################</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_hot</span><span class="params">(x, n_class, dtype=torch.float32)</span>:</span></span><br><span class="line">    <span class="comment"># x shape: (batch) , output shape: (batch, n_class) 输出batch个n维向量</span></span><br><span class="line">    x = x.long()</span><br><span class="line">    res = torch.zeros(x.shape[<span class="number">0</span>], n_class, dtype=dtype, device=x.device)</span><br><span class="line">    <span class="comment"># torch.gather(input, dim, index)  在dim维度上取</span></span><br><span class="line">    <span class="comment"># orch.Tensor.scatter_(dim, index, src)  在dim维度上放src(按照index)</span></span><br><span class="line">    res.scatter_(<span class="number">1</span>, x.view(<span class="number">-1</span>, <span class="number">1</span>), <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"><span class="comment"># 查看效果</span></span><br><span class="line">x = torch.tensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">one_hot(x, vocab_size) <span class="comment"># 2 * n_class</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_onehot</span><span class="params">(X, n_class)</span>:</span>  <span class="comment"># 输入变为一个序列长度的多个样本</span></span><br><span class="line">    <span class="comment"># X shape: (batch, seq_len), output: seq_len elements of (batch, n_class)</span></span><br><span class="line">    <span class="keyword">return</span> [one_hot(X[:, i], n_class) <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">1</span>])]</span><br><span class="line"><span class="comment"># 查看效果</span></span><br><span class="line">X = torch.arange(<span class="number">10</span>).view(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line">inputs = to_onehot(X, vocab_size)</span><br><span class="line">print(len(inputs), inputs[<span class="number">0</span>].shape) <span class="comment"># 5个2 * 1027的矩阵组成的列表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">########################### 初始化参数  ##########################</span></span><br><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line">print(<span class="string">'will use'</span>, device)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        ts = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=shape), device=device, dtype=torch.float32)</span><br><span class="line">        <span class="keyword">return</span> torch.nn.Parameter(ts, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 隐藏层参数</span></span><br><span class="line">    W_xh = _one((num_inputs, num_hiddens))</span><br><span class="line">    W_hh = _one((num_hiddens, num_hiddens))</span><br><span class="line">    b_h = torch.nn.Parameter(torch.zeros(num_hiddens, device=device, requires_grad=<span class="literal">True</span>))</span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, requires_grad=<span class="literal">True</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.ParameterList([W_xh, W_hh, b_h, W_hq, b_q])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在随机采样中需要将隐藏层参数置零</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_rnn_state</span><span class="params">(batch_size, num_hiddens, device)</span>:</span></span><br><span class="line">    <span class="comment"># 初始化隐藏层状态（参数）</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br><span class="line"></span><br><span class="line"><span class="comment">############# RNN计算 ###########</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn</span><span class="params">(inputs, state, params)</span>:</span></span><br><span class="line">    <span class="comment"># inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵</span></span><br><span class="line">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        <span class="comment"># 一个简单的RNN，隐藏层与输入X和上一个时间步的隐藏层都有关</span></span><br><span class="line">        H = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)</span><br><span class="line">        Y = torch.matmul(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="comment"># 返回输出的的批次向量列表 和 隐藏层参数</span></span><br><span class="line">    <span class="keyword">return</span> outputs, (H,)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向预测，prefix为前缀字符串，基于它预测后面的</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_rnn</span><span class="params">(prefix, num_chars, rnn, params, init_rnn_state,</span></span></span><br><span class="line"><span class="function"><span class="params">                num_hiddens, vocab_size, device, idx_to_char, char_to_idx)</span>:</span></span><br><span class="line">    state = init_rnn_state(<span class="number">1</span>, num_hiddens, device) <span class="comment"># batch为1</span></span><br><span class="line">    output = [char_to_idx[prefix[<span class="number">0</span>]]]</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_chars + len(prefix) - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 将上一时间步的输出作为当前时间步的输入</span></span><br><span class="line">        X = to_onehot(torch.tensor([[output[<span class="number">-1</span>]]], device=device), vocab_size)</span><br><span class="line">        <span class="comment"># 计算输出和更新隐藏状态</span></span><br><span class="line">        (Y, state) = rnn(X, state, params)</span><br><span class="line">        <span class="comment"># 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符</span></span><br><span class="line">        <span class="keyword">if</span> t &lt; len(prefix) - <span class="number">1</span>:</span><br><span class="line">            output.append(char_to_idx[prefix[t + <span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output.append(int(Y[<span class="number">0</span>].argmax(dim=<span class="number">1</span>).item()))</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> output])</span><br><span class="line"><span class="comment"># 查看效果</span></span><br><span class="line">predict_rnn(<span class="string">'分开'</span>, <span class="number">10</span>, rnn, params, init_rnn_state, num_hiddens, vocab_size,</span><br><span class="line">            device, idx_to_char, char_to_idx)</span><br><span class="line"></span><br><span class="line"><span class="comment">####################### 模型训练 ######################</span></span><br><span class="line"><span class="comment"># 梯度裁剪</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_clipping</span><span class="params">(params, theta, device)</span>:</span></span><br><span class="line">    <span class="comment"># 防止梯度爆炸</span></span><br><span class="line">    norm = torch.tensor([<span class="number">0.0</span>], device=device)</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        norm += (param.grad.data ** <span class="number">2</span>).sum()</span><br><span class="line">    norm = norm.sqrt().item()  <span class="comment"># 梯度的2范数</span></span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param.grad.data *= (theta / norm) <span class="comment"># 减小梯度</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, lr, batch_size)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.data -= lr * param.grad / batch_size   <span class="comment"># 更新时用.data 以免操作计入计算图</span></span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_predict_rnn</span><span class="params">(rnn, get_params, init_rnn_state, num_hiddens,</span></span></span><br><span class="line"><span class="function"><span class="params">                          vocab_size, device, corpus_indices, idx_to_char,</span></span></span><br><span class="line"><span class="function"><span class="params">                          char_to_idx, is_random_iter, num_epochs, num_steps,</span></span></span><br><span class="line"><span class="function"><span class="params">                          lr, clipping_theta, batch_size, pred_period,</span></span></span><br><span class="line"><span class="function"><span class="params">                          pred_len, prefixes)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> is_random_iter:</span><br><span class="line">        data_iter_fn = data_iter_random</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        data_iter_fn = data_iter_consecutive</span><br><span class="line">    params = get_params()</span><br><span class="line">    loss = nn.CrossEntropyLoss()  </span><br><span class="line">    <span class="comment">### 交叉熵损失函数，由logsoftmax和NLLloss结合，先对one-hot计算logsoftmax，再通过NLLloss与标签</span></span><br><span class="line">    <span class="comment">### 对比，目标是分类概率最大。其参数前面是(x, y) ,x是预测的one-hot型数据，形状是(batch, vocab_size)</span></span><br><span class="line">    <span class="comment">### y是标签，(batch)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_random_iter:  <span class="comment"># 如使用相邻采样，在epoch开始时初始化隐藏状态</span></span><br><span class="line">            state = init_rnn_state(batch_size, num_hiddens, device)</span><br><span class="line">        l_sum, n, start = <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, device)</span><br><span class="line">        <span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> is_random_iter:  <span class="comment"># 如使用随机采样，在每个小批量更新前初始化隐藏状态</span></span><br><span class="line">                state = init_rnn_state(batch_size, num_hiddens, device)</span><br><span class="line">            <span class="keyword">else</span>:  </span><br><span class="line">            <span class="comment"># 否则需要使用detach函数从计算图分离隐藏状态, 这是为了</span></span><br><span class="line">            <span class="comment"># 使模型参数的梯度计算只依赖一次迭代读取的小批量序列(防止梯度计算开销太大)</span></span><br><span class="line">                <span class="keyword">for</span> s <span class="keyword">in</span> state: <span class="comment"># h</span></span><br><span class="line">                    s.detach_()</span><br><span class="line"></span><br><span class="line">            inputs = to_onehot(X, vocab_size)</span><br><span class="line">            <span class="comment"># outputs有num_steps个形状为(batch_size, vocab_size)的矩阵</span></span><br><span class="line">            (outputs, state) = rnn(inputs, state, params)</span><br><span class="line">            <span class="comment"># 拼接之后形状为(num_steps * batch_size, vocab_size)</span></span><br><span class="line">            outputs = torch.cat(outputs, dim=<span class="number">0</span>)</span><br><span class="line">            <span class="comment"># Y的形状是(batch_size, num_steps)，转置后再变成长度为</span></span><br><span class="line">            <span class="comment"># batch * num_steps 的向量，这样跟输出的行一一对应</span></span><br><span class="line">            y = torch.transpose(Y, <span class="number">0</span>, <span class="number">1</span>).contiguous().view(<span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># 使用交叉熵损失计算平均分类误差</span></span><br><span class="line">            l = loss(outputs, y.long())</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 梯度清0</span></span><br><span class="line">            <span class="keyword">if</span> params[<span class="number">0</span>].grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">                    param.grad.data.zero_()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(params, clipping_theta, device)  <span class="comment"># 裁剪梯度</span></span><br><span class="line">            sgd(params, lr, <span class="number">1</span>)  <span class="comment"># 因为误差已经取过均值，梯度不用再做平均</span></span><br><span class="line">            l_sum += l.item() * y.shape[<span class="number">0</span>]</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % pred_period == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'epoch %d, perplexity %f, time %.2f sec'</span> % (</span><br><span class="line">                epoch + <span class="number">1</span>, math.exp(l_sum / n), time.time() - start))</span><br><span class="line">            <span class="keyword">for</span> prefix <span class="keyword">in</span> prefixes:</span><br><span class="line">                print(<span class="string">' -'</span>, predict_rnn(prefix, pred_len, rnn, params, init_rnn_state,</span><br><span class="line">                    num_hiddens, vocab_size, device, idx_to_char, char_to_idx))</span><br><span class="line">                </span><br><span class="line">                </span><br><span class="line"><span class="comment">############ 测试训练 </span></span><br><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">250</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">50</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机采样，相邻采样设置False即可</span></span><br><span class="line">train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,</span><br><span class="line">                      vocab_size, device, corpus_indices, idx_to_char,</span><br><span class="line">                      char_to_idx, <span class="literal">True</span>, num_epochs, num_steps, lr,</span><br><span class="line">                      clipping_theta, batch_size, pred_period, pred_len,</span><br><span class="line">                      prefixes)</span><br></pre></td></tr></table></figure>
<h3 id="torch函数实现"><a href="#torch函数实现" class="headerlink" title="torch函数实现"></a>torch函数实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens , num_steps, batch_size= <span class="number">256</span>, <span class="number">35</span>, <span class="number">2</span></span><br><span class="line"><span class="comment"># rnn_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens) # 已测试</span></span><br><span class="line">rnn_layer = nn.RNN(input_size=vocab_size, hidden_size=num_hiddens)</span><br><span class="line"><span class="comment"># torch的RNN输入数据形状是(seq_len, batch_size, vocab_size)</span></span><br><span class="line"></span><br><span class="line">state = <span class="literal">None</span></span><br><span class="line">X = torch.rand(num_steps, batch_size, vocab_size)</span><br><span class="line">Y, state_new = rnn_layer(X, state)</span><br><span class="line">print(Y.shape, len(state_new), state_new[<span class="number">0</span>].shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">################ RNN模型，使用内置RNN_layer</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_layer, vocab_size)</span>:</span></span><br><span class="line">        super(RNNModel, self).__init__()</span><br><span class="line">        self.rnn = rnn_layer</span><br><span class="line">        self.hidden_size = rnn_layer.hidden_size * (<span class="number">2</span> <span class="keyword">if</span> rnn_layer.bidirectional <span class="keyword">else</span> <span class="number">1</span>) </span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.dense = nn.Linear(self.hidden_size, vocab_size)</span><br><span class="line">        self.state = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, state)</span>:</span> <span class="comment"># inputs: (batch, seq_len)</span></span><br><span class="line">        <span class="comment"># 获取one-hot向量表示</span></span><br><span class="line">        X = to_onehot(inputs, self.vocab_size) <span class="comment"># X是个以seq_len为索引长度的list</span></span><br><span class="line">        Y, self.state = self.rnn(torch.stack(X), state)  </span><br><span class="line">        <span class="comment"># Y.shape (seq_len, batch, num_hiddens)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 先将Y的形状变成(num_steps * batch_size, num_hiddens)再输入全连接层，经过它计算后的输出</span></span><br><span class="line">        <span class="comment"># 形状为(num_steps * batch_size, vocab_size)</span></span><br><span class="line">        output = self.dense(Y.view(<span class="number">-1</span>, Y.shape[<span class="number">-1</span>]))  <span class="comment">## </span></span><br><span class="line">        <span class="keyword">return</span> output, self.state</span><br><span class="line"></span><br><span class="line"><span class="comment">########  前向预测  #############</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_rnn_pytorch</span><span class="params">(prefix, num_chars, model, vocab_size, device, idx_to_char,</span></span></span><br><span class="line"><span class="function"><span class="params">                      char_to_idx)</span>:</span></span><br><span class="line">    state = <span class="literal">None</span></span><br><span class="line">    output = [char_to_idx[prefix[<span class="number">0</span>]]] <span class="comment"># output会记录prefix加上输出</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_chars + len(prefix) - <span class="number">1</span>):</span><br><span class="line">        X = torch.tensor([output[<span class="number">-1</span>]], device=device).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> isinstance(state, tuple): <span class="comment"># LSTM, state:(h, c)  </span></span><br><span class="line">                state = (state[<span class="number">0</span>].to(device), state[<span class="number">1</span>].to(device))</span><br><span class="line">            <span class="keyword">else</span>:   </span><br><span class="line">                state = state.to(device)</span><br><span class="line"></span><br><span class="line">        (Y, state) = model(X, state)</span><br><span class="line">        <span class="keyword">if</span> t &lt; len(prefix) - <span class="number">1</span>:</span><br><span class="line">            output.append(char_to_idx[prefix[t + <span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output.append(int(Y.argmax(dim=<span class="number">1</span>).item()))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> output])</span><br><span class="line"></span><br><span class="line"><span class="comment">############################## 模型训练 #################</span></span><br><span class="line">model = RNNModel(rnn_layer, vocab_size).to(device)</span><br><span class="line"><span class="comment"># 使用初始化数据随机预测10个看看</span></span><br><span class="line">predict_rnn_pytorch(<span class="string">'分开'</span>, <span class="number">10</span>, model, vocab_size, device, idx_to_char, char_to_idx)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_predict_rnn_pytorch</span><span class="params">(model, num_hiddens, vocab_size, device,</span></span></span><br><span class="line"><span class="function"><span class="params">                                corpus_indices, idx_to_char, char_to_idx,</span></span></span><br><span class="line"><span class="function"><span class="params">                                num_epochs, num_steps, lr, clipping_theta,</span></span></span><br><span class="line"><span class="function"><span class="params">                                batch_size, pred_period, pred_len, prefixes)</span>:</span></span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=lr)</span><br><span class="line">    model.to(device)</span><br><span class="line">    state = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        l_sum, n, start = <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        data_iter = data_iter_consecutive(corpus_indices, batch_size, num_steps, device) <span class="comment"># 相邻采样</span></span><br><span class="line">        <span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> state <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 使用detach函数从计算图分离隐藏状态, 这是为了</span></span><br><span class="line">                <span class="comment"># 使模型参数的梯度计算只依赖一次迭代读取的小批量序列(防止梯度计算开销太大)</span></span><br><span class="line">                <span class="keyword">if</span> isinstance (state, tuple): <span class="comment"># LSTM, state:(h, c)  </span></span><br><span class="line">                    state = (state[<span class="number">0</span>].detach(), state[<span class="number">1</span>].detach())</span><br><span class="line">                <span class="keyword">else</span>:   </span><br><span class="line">                    state = state.detach()</span><br><span class="line"></span><br><span class="line">            (output, state) = model(X, state) <span class="comment"># output: 形状为(num_steps * batch_size, vocab_size)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Y的形状是(batch_size, num_steps)，转置后再变成长度为</span></span><br><span class="line">            <span class="comment"># batch * num_steps 的向量，这样跟输出的行一一对应</span></span><br><span class="line">            y = torch.transpose(Y, <span class="number">0</span>, <span class="number">1</span>).contiguous().view(<span class="number">-1</span>)</span><br><span class="line">            l = loss(output, y.long())</span><br><span class="line"></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            <span class="comment"># 梯度裁剪</span></span><br><span class="line">            grad_clipping(model.parameters(), clipping_theta, device)</span><br><span class="line">            optimizer.step()</span><br><span class="line">            l_sum += l.item() * y.shape[<span class="number">0</span>]</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            perplexity = math.exp(l_sum / n)</span><br><span class="line">        <span class="keyword">except</span> OverflowError:</span><br><span class="line">            perplexity = float(<span class="string">'inf'</span>)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % pred_period == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'epoch %d, perplexity %f, time %.2f sec'</span> % (</span><br><span class="line">                epoch + <span class="number">1</span>, perplexity, time.time() - start))</span><br><span class="line">            <span class="keyword">for</span> prefix <span class="keyword">in</span> prefixes:</span><br><span class="line">                print(<span class="string">' -'</span>, predict_rnn_pytorch(</span><br><span class="line">                    prefix, pred_len, model, vocab_size, device, idx_to_char,</span><br><span class="line">                    char_to_idx))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置参数训练</span></span><br><span class="line">num_epochs, batch_size, lr, clipping_theta = <span class="number">250</span>, <span class="number">32</span>, <span class="number">1e-3</span>, <span class="number">1e-2</span> <span class="comment"># 注意这里的学习率设置</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">50</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line">train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class="line">                            corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                            num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                            batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure>
<h2 id="GRU门控单元"><a href="#GRU门控单元" class="headerlink" title="GRU门控单元"></a>GRU门控单元</h2><p><img src="1594893699477.png" alt="1594893699477"></p>
<p>重置门和更新门的输入均为当前时间步的输入$X_t$与上一时间步的隐藏状态$H_{t-1}$， 激活函数都是sigmoid函数，因此它们的输出值域是$(0, 1)$，重置门的输出$R_t$用于得到候选隐藏状态，而更新门的输出$Z_t$用于在得到候选隐藏状态后确定该隐藏状态。</p>
<p>其中，候选隐藏状态计算为：</p>
<script type="math/tex; mode=display">
\tilde H_t = tanh(X_tW_{xh}+(R_t \odot H_{t-1}) W_{hh} + b_h)</script><p>其中$\odot$是元素乘法， $R_t$在$(0, 1)$之间，因此重置门的作用可以看作用来筛选之前的历史信息$H_{t-1}$。</p>
<p>隐藏状态的计算为：</p>
<script type="math/tex; mode=display">
H_t = Z_t\odot H_{t-1} + (1 - Z_t)\odot \tilde H_t</script><p>可见更新门在候选状态与上一时间步的状态作取舍，它的作用可以看作保存之前时刻的状态。</p>
<p>这样这两个的门的设计：重置门可理解为获取短期依赖关系，更新门可理解为获取长期依赖关系。</p>
<h3 id="手动实现-1"><a href="#手动实现-1" class="headerlink" title="手动实现"></a>手动实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line">print(<span class="string">'will use'</span>, device)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        ts = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=shape), device=device, dtype=torch.float32)</span><br><span class="line">        <span class="keyword">return</span> torch.nn.Parameter(ts, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_three</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (_one((num_inputs, num_hiddens)),</span><br><span class="line">                _one((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">    W_xz, W_hz, b_z = _three()  <span class="comment"># 更新门参数</span></span><br><span class="line">    W_xr, W_hr, b_r = _three()  <span class="comment"># 重置门参数</span></span><br><span class="line">    W_xh, W_hh, b_h = _three()  <span class="comment"># 候选隐藏状态参数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> nn.ParameterList([W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化隐藏状态参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_gru_state</span><span class="params">(batch_size, num_hiddens, device)</span>:</span>  </span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br></pre></td></tr></table></figure>
<p>定义GRU计算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gru</span><span class="params">(inputs, state, params)</span>:</span></span><br><span class="line">    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        Z = torch.sigmoid(torch.matmul(X, W_xz) + torch.matmul(H, W_hz) + b_z)</span><br><span class="line">        R = torch.sigmoid(torch.matmul(X, W_xr) + torch.matmul(H, W_hr) + b_r)</span><br><span class="line">        H_tilda = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(R * H, W_hh) + b_h)</span><br><span class="line">        H = Z * H + (<span class="number">1</span> - Z) * H_tilda</span><br><span class="line">        Y = torch.matmul(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> outputs, (H,)</span><br></pre></td></tr></table></figure>
<p>直接使用之间的训练函数训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">160</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">40</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line"></span><br><span class="line">train_and_predict_rnn(gru, get_params, init_gru_state, num_hiddens,</span><br><span class="line">                          vocab_size, device, corpus_indices, idx_to_char,</span><br><span class="line">                          char_to_idx, <span class="literal">False</span>, num_epochs, num_steps, lr,</span><br><span class="line">                          clipping_theta, batch_size, pred_period, pred_len,</span><br><span class="line">                          prefixes)</span><br></pre></td></tr></table></figure>
<h3 id="torch函数实现-1"><a href="#torch函数实现-1" class="headerlink" title="torch函数实现"></a>torch函数实现</h3><p>直接调用GRU层即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">1e-2</span> <span class="comment"># 注意调整学习率</span></span><br><span class="line">gru_layer = nn.GRU(input_size=vocab_size, hidden_size=num_hiddens)</span><br><span class="line">model = RNNModel(gru_layer, vocab_size).to(device)</span><br><span class="line">train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class="line">                                corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                                num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                                batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure>
<h2 id="长短期记忆"><a href="#长短期记忆" class="headerlink" title="长短期记忆"></a>长短期记忆</h2><p>长短期记忆（long short-term memory，LSTM）中包含三个门，输入门（input gate）、遗忘门（forget gate）和输出门（output gate），以及与隐藏状态形状相同的记忆细胞。</p>
<p><img src="1595067353701.png" alt="1595067353701"></p>
<p>上图可以清晰的看出LSTM单元的计算过程，其中三个门控单元都是由上一时间步的隐藏状态和当前时间步的输入通过sigmoid函数得到，候选记忆细胞与它们的区别只是使用了tanh函数。然后通过这四个单元计算当前时间步的记忆细胞$C_t$和隐藏状态$H_t$。</p>
<script type="math/tex; mode=display">
C_t = F_t \odot C_{t-1} + I_t \odot \bar C_t\\
H_t = O_t \odot tanh(C_t)</script><h3 id="手动实现-2"><a href="#手动实现-2" class="headerlink" title="手动实现"></a>手动实现</h3><ul>
<li>获取参数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line">print(<span class="string">'will use'</span>, device)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        ts = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=shape), device=device, dtype=torch.float32)</span><br><span class="line">        <span class="keyword">return</span> torch.nn.Parameter(ts, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_three</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (_one((num_inputs, num_hiddens)),</span><br><span class="line">                _one((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">    W_xi, W_hi, b_i = _three()  <span class="comment"># 输入门参数</span></span><br><span class="line">    W_xf, W_hf, b_f = _three()  <span class="comment"># 遗忘门参数</span></span><br><span class="line">    W_xo, W_ho, b_o = _three()  <span class="comment"># 输出门参数</span></span><br><span class="line">    W_xc, W_hc, b_c = _three()  <span class="comment"># 候选记忆细胞参数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> nn.ParameterList([W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q])</span><br></pre></td></tr></table></figure>
<ul>
<li>初始化变量</li>
</ul>
<p>LSTM会额外返回一个形状与隐藏状态相同的记忆细胞</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_lstm_state</span><span class="params">(batch_size, num_hiddens, device)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), </span><br><span class="line">            torch.zeros((batch_size, num_hiddens), device=device))</span><br></pre></td></tr></table></figure>
<ul>
<li>LSTM计算</li>
</ul>
<p>其中记忆细胞$C_t$只在隐藏层间计算，不流动到输出层，输出层只计算隐藏状态。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm</span><span class="params">(inputs, state, params)</span>:</span></span><br><span class="line">    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q] = params</span><br><span class="line">    (H, C) = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        I = torch.sigmoid(torch.matmul(X, W_xi) + torch.matmul(H, W_hi) + b_i)</span><br><span class="line">        F = torch.sigmoid(torch.matmul(X, W_xf) + torch.matmul(H, W_hf) + b_f)</span><br><span class="line">        O = torch.sigmoid(torch.matmul(X, W_xo) + torch.matmul(H, W_ho) + b_o)</span><br><span class="line">        C_tilda = torch.tanh(torch.matmul(X, W_xc) + torch.matmul(H, W_hc) + b_c)</span><br><span class="line">        C = F * C + I * C_tilda</span><br><span class="line">        H = O * C.tanh()</span><br><span class="line">        Y = torch.matmul(H, W_hq) + b_q    <span class="comment"># 输出层</span></span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> outputs, (H, C)</span><br></pre></td></tr></table></figure>
<ul>
<li>训练</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">160</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">40</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line"></span><br><span class="line">train_and_predict_rnn(lstm, get_params, init_lstm_state, num_hiddens,</span><br><span class="line">                          vocab_size, device, corpus_indices, idx_to_char,</span><br><span class="line">                          char_to_idx, <span class="literal">False</span>, num_epochs, num_steps, lr,</span><br><span class="line">                          clipping_theta, batch_size, pred_period, pred_len,</span><br><span class="line">                          prefixes)</span><br></pre></td></tr></table></figure>
<h3 id="torch函数实现-2"><a href="#torch函数实现-2" class="headerlink" title="torch函数实现"></a>torch函数实现</h3><p>调用即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">1e-2</span> <span class="comment"># 注意调整学习率</span></span><br><span class="line">lstm_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens)</span><br><span class="line">model = RNNModel(lstm_layer, vocab_size)</span><br><span class="line">d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class="line">                                corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                                num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                                batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="../../pytorch/各类数据表示/" rel="next" title>
                <i class="fa fa-chevron-left"></i> 
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="../7.卷积网络/" rel="prev" title>
                 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">brezezee</p>
              <p class="site-description motion-element" itemprop="description">靡不有初，鲜克有终</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="https://brezezee.github.io/archives">
              
                  <span class="site-state-item-count">140</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="../../../../../../categories/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="../../../../../../tags/index.html">
                  <span class="site-state-item-count">43</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#基础结构"><span class="nav-text">基础结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度计算"><span class="nav-text">梯度计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#读取数据集"><span class="nav-text">读取数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#采样时序数据"><span class="nav-text">采样时序数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基础RNN"><span class="nav-text">基础RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#手动实现"><span class="nav-text">手动实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#torch函数实现"><span class="nav-text">torch函数实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GRU门控单元"><span class="nav-text">GRU门控单元</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#手动实现-1"><span class="nav-text">手动实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#torch函数实现-1"><span class="nav-text">torch函数实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#长短期记忆"><span class="nav-text">长短期记忆</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#手动实现-2"><span class="nav-text">手动实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#torch函数实现-2"><span class="nav-text">torch函数实现</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">brezezee</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">181k</span>
  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="../../../../../../lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="../../../../../../lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="../../../../../../lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="../../../../../../lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="../../../../../../lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="../../../../../../lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="../../../../../../js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="../../../../../../js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="../../../../../../js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="../../../../../../js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="../../../../../../js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'https://brezezee.github.io/2020/09/09/framework/DL with pytorch/8.循环网络/';
          this.page.identifier = '2020/09/09/framework/DL with pytorch/8.循环网络/';
          this.page.title = '';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
