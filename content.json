[{"title":"python基础语法（一）","date":"2019-08-27T08:28:29.263Z","path":"2019/08/27/python基础1/","excerpt":"","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/source/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/source/tags/python/"}]},{"title":"剑指offer","date":"2019-08-27T08:27:51.773Z","path":"2019/08/27/剑指offer/","excerpt":"","categories":[{"name":"刷题","slug":"刷题","permalink":"http://yoursite.com/source/categories/刷题/"}],"tags":[{"name":"剑指offer","slug":"剑指offer","permalink":"http://yoursite.com/source/tags/剑指offer/"}]},{"title":"拉格朗日对偶性(lagrange duality)","date":"2019-08-14T05:05:35.080Z","path":"2019/08/14/拉格朗日对偶性(Lagrange duality)/","excerpt":"拉格朗日对偶性(Lagrange duality)1. 从原始问题到对偶问题&emsp;对偶性是优化理论中一个重要的部分，带约束的优化问题是机器学习中经常遇到的问题，这类问题都可以用如下形式表达 \\begin{aligned} min \\;\\; &f(x) \\\\ s.t.\\;\\; & g_i(x) \\le 0 ,\\;\\; i=1,\\cdots, m\\\\ & h_i(x) = 0,\\;\\; i=1,\\cdots,n\\\\ \\end{aligned}约束条件减少需要求解的空间，但在机器学习中，约束条件往往比较复杂并且较多。因此先计算约束条件再在约束空间中计算最优值非常不方便。于是用广义拉格朗日函数将带约束优化问题转化为无约束优化问题","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/source/categories/机器学习/"}],"tags":[{"name":"lagrange duality","slug":"lagrange-duality","permalink":"http://yoursite.com/source/tags/lagrange-duality/"}]},{"title":"SVM推导","date":"2019-08-14T05:05:34.808Z","path":"2019/08/14/SVM推导/","excerpt":"&emsp;&emsp;支持向量机是一种二分类模型，它以间隔最大作为优化目标，因此它比感知机仅仅以误分类推动的模型要优胜不少。数据线性可分时，SVM直接以间隔最大训练出一个线性分类模型。当数据线性不可分时，SVM通过软间隔最大化解决这种问题。此时的模型仍是一个线性模型，若采用核方法和软间隔最大化，则得到一个非线性模型。可以看到SVM在解决变复杂的情况时是逐步改进的。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/source/categories/机器学习/"}],"tags":[{"name":"SVM, SMO","slug":"SVM-SMO","permalink":"http://yoursite.com/source/tags/SVM-SMO/"}]},{"title":"Hexo+github博客搭建","date":"2019-08-14T02:55:44.811Z","path":"2019/08/14/Hexo + github 博客搭建/","excerpt":"Hexo + github 博客搭建1. 环境要求 需要github desktop，可进入github官网下载。 需要nodejs。可进入nodejs.org 下载LTS版nodejs安装。 安装好后在命令行（windows在CMD）或gitbash输入，win推荐gitbash，命令和linux差不多。","categories":[{"name":"环境搭建","slug":"环境搭建","permalink":"http://yoursite.com/source/categories/环境搭建/"}],"tags":[{"name":"hexo, 博客搭建","slug":"hexo-博客搭建","permalink":"http://yoursite.com/source/tags/hexo-博客搭建/"}]},{"title":"熵、交叉熵及似然函数的关系","date":"2019-07-30T13:03:11.124Z","path":"2019/07/30/熵、交叉熵、极大似然/","excerpt":"熵、交叉熵及似然函数的关系1. 熵1.1 信息量&emsp;&emsp;信息量：最初的定义是信号取值数量m的对数为信息量$I$,即 $I=log_2m$。这是与比特数相关的，比如一个信号只有两个取值，那么用1个bit便能将其表示。后来著名的香农指出，得到的信号取值结果是随机的，因此其信息量应该也是关于概率的函数，于是得到随机变量$X$的信息量","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/source/categories/机器学习/"}],"tags":[{"name":"熵, 交叉熵, 似然函数","slug":"熵-交叉熵-似然函数","permalink":"http://yoursite.com/source/tags/熵-交叉熵-似然函数/"}]},{"title":"logistic回归和最大熵模型","date":"2019-07-29T05:40:17.533Z","path":"2019/07/29/6.logistic回归和最大熵模型/","excerpt":"logistic回归和最大熵模型1. logistic回归模型&emsp;&emsp;logistic回归是一种广义线性回归（generalized linear model），因此与多重线性回归分析有很多相同之处。它们的模型形式基本上相同，都具有wx+b，其中w和b是待求参数，其区别在于他们的因变量不同，多重线性回归直接将wx+b作为因变量，即y =wx+b，而logistic回归则通过函数g(wx+b)让其对应一个隐状态p，p =g(wx+b),然后根据p 与1-p的比值大小（几率）决定因变量的值。如果g是logistic函数，就是logistic回归，如果g是多项式函数就是多项式回归。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/source/categories/机器学习/"}],"tags":[{"name":"logistic, 最大熵模型","slug":"logistic-最大熵模型","permalink":"http://yoursite.com/source/tags/logistic-最大熵模型/"}]}]