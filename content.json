[{"title":"python基础语法（一）","date":"2019-08-27T08:28:29.263Z","path":"2019/08/27/python基础1/","excerpt":"","categories":[{"name":"python","slug":"python","permalink":"https://brezezee.github.io/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://brezezee.github.io/tags/python/"}]},{"title":"剑指offer","date":"2019-08-27T08:27:51.773Z","path":"2019/08/27/剑指offer/","excerpt":"","categories":[{"name":"刷题","slug":"code-practice","permalink":"https://brezezee.github.io/categories/code-practice/"}],"tags":[{"name":"剑指offer","slug":"剑指offer","permalink":"https://brezezee.github.io/tags/剑指offer/"}]},{"title":"条件随机场","date":"2019-08-22T11:11:54.000Z","path":"2019/08/22/条件随机场/","excerpt":"","categories":[{"name":"机器学习","slug":"meachine-learning","permalink":"https://brezezee.github.io/categories/meachine-learning/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://brezezee.github.io/tags/机器学习/"},{"name":"条件随机场","slug":"条件随机场","permalink":"https://brezezee.github.io/tags/条件随机场/"},{"name":"概率图模型","slug":"概率图模型","permalink":"https://brezezee.github.io/tags/概率图模型/"}]},{"title":"SVM推导","date":"2019-08-18T11:32:12.000Z","path":"2019/08/18/SVM推导/","excerpt":"这篇文章的目录乱了，本地是对的，也没有跳级编号。不知道是什么情况，有人知道如何解决的话还请告知一声，强迫症看着很难受 &emsp;&emsp;支持向量机是一种二分类模型，它以间隔最大作为优化目标，因此它比感知机仅仅以误分类推动的模型要优胜不少。数据线性可分时，SVM直接以间隔最大训练出一个线性分类模型。当数据线性不可分时，SVM通过软间隔最大化解决这种问题。此时的模型仍是一个线性模型，若采用核方法和软间隔最大化，则得到一个非线性模型。可以看到SVM在解决变复杂的情况时是逐步改进的。","categories":[{"name":"机器学习","slug":"meachine-learning","permalink":"https://brezezee.github.io/categories/meachine-learning/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://brezezee.github.io/tags/机器学习/"},{"name":"SVM","slug":"SVM","permalink":"https://brezezee.github.io/tags/SVM/"},{"name":"SMO","slug":"SMO","permalink":"https://brezezee.github.io/tags/SMO/"}]},{"title":"熵、交叉熵及似然函数的关系","date":"2019-08-11T16:21:53.000Z","path":"2019/08/12/熵、交叉熵、极大似然/","excerpt":"1. 熵1.1 信息量&emsp;&emsp;信息量：最初的定义是信号取值数量m的对数为信息量$I$,即 $I=log_2m$。这是与比特数相关的，比如一个信号只有两个取值，那么用1个bit便能将其表示。后来著名的香农指出，得到的信号取值结果是随机的，因此其信息量应该也是关于概率的函数，于是得到随机变量$X$的信息量","categories":[{"name":"机器学习","slug":"meachine-learning","permalink":"https://brezezee.github.io/categories/meachine-learning/"}],"tags":[{"name":"交叉熵","slug":"交叉熵","permalink":"https://brezezee.github.io/tags/交叉熵/"},{"name":"似然函数","slug":"似然函数","permalink":"https://brezezee.github.io/tags/似然函数/"}]},{"title":"隐马尔可夫模型","date":"2019-08-11T11:11:54.000Z","path":"2019/08/11/隐马尔可夫模型/","excerpt":"","categories":[{"name":"机器学习","slug":"meachine-learning","permalink":"https://brezezee.github.io/categories/meachine-learning/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://brezezee.github.io/tags/机器学习/"},{"name":"概率图模型","slug":"概率图模型","permalink":"https://brezezee.github.io/tags/概率图模型/"},{"name":"隐马尔可夫模型","slug":"隐马尔可夫模型","permalink":"https://brezezee.github.io/tags/隐马尔可夫模型/"}]},{"title":"拉格朗日对偶性(lagrange duality)","date":"2019-08-03T17:22:21.000Z","path":"2019/08/04/拉格朗日对偶性(Lagrange duality)/","excerpt":"1. 从原始问题到对偶问题&emsp;对偶性是优化理论中一个重要的部分，带约束的优化问题是机器学习中经常遇到的问题，这类问题都可以用如下形式表达 \\begin{aligned} min \\;\\; &f(x) \\\\ s.t.\\;\\; & g_i(x) \\le 0 ,\\;\\; i=1,\\cdots, m\\\\ & h_i(x) = 0,\\;\\; i=1,\\cdots,n\\\\ \\end{aligned}约束条件减少需要求解的空间，但在机器学习中，约束条件往往比较复杂并且较多。因此先计算约束条件再在约束空间中计算最优值非常不方便。于是用广义拉格朗日函数将带约束优化问题转化为无约束优化问题","categories":[{"name":"机器学习","slug":"meachine-learning","permalink":"https://brezezee.github.io/categories/meachine-learning/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://brezezee.github.io/tags/机器学习/"},{"name":"lagrange duality","slug":"lagrange-duality","permalink":"https://brezezee.github.io/tags/lagrange-duality/"}]},{"title":"Hexo+github博客优化","date":"2019-07-31T17:32:12.000Z","path":"2019/08/01/Hexo+github博客优化/","excerpt":"","categories":[{"name":"环境搭建","slug":"environment-config","permalink":"https://brezezee.github.io/categories/environment-config/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://brezezee.github.io/tags/hexo/"},{"name":"博客搭建","slug":"博客搭建","permalink":"https://brezezee.github.io/tags/博客搭建/"}]},{"title":"EM算法","date":"2019-07-25T06:12:12.000Z","path":"2019/07/25/EM算法/","excerpt":"","categories":[{"name":"机器学习","slug":"meachine-learning","permalink":"https://brezezee.github.io/categories/meachine-learning/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://brezezee.github.io/tags/机器学习/"},{"name":"EM算法","slug":"EM算法","permalink":"https://brezezee.github.io/tags/EM算法/"}]},{"title":"logistic回归和最大熵模型","date":"2019-07-23T16:12:21.000Z","path":"2019/07/24/6.logistic回归和最大熵模型/","excerpt":"1. logistic回归模型&emsp;&emsp;logistic回归是一种广义线性回归（generalized linear model），因此与多重线性回归分析有很多相同之处。它们的模型形式基本上相同，都具有wx+b，其中w和b是待求参数，其区别在于他们的因变量不同，多重线性回归直接将wx+b作为因变量，即y =wx+b，而logistic回归则通过函数g(wx+b)让其对应一个隐状态p，p =g(wx+b),然后根据p 与1-p的比值大小（几率）决定因变量的值。如果g是logistic函数，就是logistic回归，如果g是多项式函数就是多项式回归。","categories":[{"name":"机器学习","slug":"meachine-learning","permalink":"https://brezezee.github.io/categories/meachine-learning/"}],"tags":[{"name":"logistic","slug":"logistic","permalink":"https://brezezee.github.io/tags/logistic/"},{"name":"最大熵模型","slug":"最大熵模型","permalink":"https://brezezee.github.io/tags/最大熵模型/"}]},{"title":"Hexo+github博客搭建","date":"2019-07-12T04:32:12.000Z","path":"2019/07/12/Hexo + github 博客搭建/","excerpt":"Hexo + github 博客搭建1. 环境要求 需要github desktop，可进入github官网下载。 需要nodejs。可进入nodejs.org 下载LTS版nodejs安装。 安装好后在命令行（windows在CMD）或gitbash输入，win推荐gitbash，命令和linux差不多。","categories":[{"name":"环境搭建","slug":"environment-config","permalink":"https://brezezee.github.io/categories/environment-config/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://brezezee.github.io/tags/hexo/"},{"name":"博客搭建","slug":"博客搭建","permalink":"https://brezezee.github.io/tags/博客搭建/"}]}]