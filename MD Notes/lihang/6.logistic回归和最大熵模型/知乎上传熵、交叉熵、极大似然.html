<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="../../../lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="../../../lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="../../../css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="../../../images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="../../../images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="../../../images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="../../../images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="[TOC] 熵、交叉熵及似然函数的关系1. 熵1.1 信息量&amp;emsp;&amp;emsp;信息量：最初的定义是信号取值数量m的对数为信息量,即 。这是与比特数相关的，比如一个信号只有两个取值，那么用1个bit便能将其表示。后来著名的香农指出，得到的信号取值结果是随机的，因此其信息量应该也是关于概率的函数，于是得到随机变量的信息量  &amp;emsp;&amp;emsp;在机器学习中，熵是描述一个变量发生的不确定性的度">
<meta name="keywords" content="ml,dl,cv,ai">
<meta property="og:type" content="website">
<meta property="og:title" content="brezezee">
<meta property="og:url" content="https://brezezee.github.io/MD Notes/lihang/6.logistic回归和最大熵模型/知乎上传熵、交叉熵、极大似然.html">
<meta property="og:site_name" content="brezezee">
<meta property="og:description" content="[TOC] 熵、交叉熵及似然函数的关系1. 熵1.1 信息量&amp;emsp;&amp;emsp;信息量：最初的定义是信号取值数量m的对数为信息量,即 。这是与比特数相关的，比如一个信号只有两个取值，那么用1个bit便能将其表示。后来著名的香农指出，得到的信号取值结果是随机的，因此其信息量应该也是关于概率的函数，于是得到随机变量的信息量  &amp;emsp;&amp;emsp;在机器学习中，熵是描述一个变量发生的不确定性的度">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=I">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=I%3Dlog_2m">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=X">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=I(X%3Dx_i)%20%3D%20-logP(x_i)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=H(X)%3DE_P[-logP(x)]%3D-%5Csum_xp(x_i)logp(x_i)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=H(x)%20%3D%20-%5Csum_n%20%5Cfrac%201%20n%20log%20%5Cfrac%201%20n%3D%20logn">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=logn">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L_%7B%5Cwidetilde%20P%7D%20%3D%20%5Cprod_%7Bx%2Cy%7D%20P(y|x)%5E%7B%5Cwidetilde%20P(x%2Cy)%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L(x_1%2C%5Ccdots%2Cx_n%2C%5Ctheta)%3D%5Cprod_x%20P(x)%5E%7B%5Cwidehat%20P(x)%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L(x%2C%5Ctheta)%3D%5Cprod%20_%7Bi%3D1%7D%5En%20P(x_i)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=n">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=X">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=m">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=v_1%2C%5Ccdots%2Cv_m">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=v_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=C(X%3Dv_i)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L(x%2C%5Ctheta)%20%3D%20%5Cprod%20_%7Bi%3D1%7D%5Em%20P(x_i)%5E%7BC(X%3Dx_i)%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=n">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L(x%2C%5Ctheta)%5E%7B%5Cfrac%201%20n%7D%20%3D%20%5Cprod%20_%7Bi%3D1%7D%20%5Em%20P(x_i)%20%5E%7B%5Cfrac%20%7BC(X%3Dx_i)%7D%20%7Bn%7D%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%20%7BC(X%3Dx_i)%7D%20%7Bn%7D%20%3D%20%5Cwidetilde%20P(x_i)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=n">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L(x%2C%5Ctheta)%20%20%3D%20%20%5Cprod%20_%7Bi%3D1%7D%20%5Em%20P(x_i)%20%5E%7B%5Cfrac%20%7BC(X%3Dx_i)%7D%20%7Bn%7D%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cwidetilde%20P(x_i)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L(x%2C%5Ctheta)%20%3D%20%5Cprod%20_x%20P(x)%20%5E%7B%5Cwidetilde%20P(x)%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L(x%2C%5Ctheta)%20%3D%20%5Cwidetilde%20P(x)%20%5Csum_x%20log%20P(x)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D
L_%7B%5Cwidetilde%20P%7D%20%3D%26%20log%5Cprod_%7Bx%2Cy%7D%20P(x%2Cy)%20%5E%7B%5Cwidetilde%20P(x%2Cy)%7D%5C%5C
%3D%26%5Csum_%7Bx%2Cy%7D%20%5Cwidetilde%20P(x%2Cy)log[%5Cwidetilde%20P(x)%20P(y|x)]%5C%5C
%3D%26%5Csum_%7Bx%2Cy%7D%20%5Cwidetilde%20P(x%2Cy)logP(y|x)%20%2B%20%5Csum_%7Bx%2Cy%7D%20%5Cwidetilde%20P(x%2Cy)%20log%20%5Cwidetilde%20P(x)%5C%5C
%3D%26%20%5Csum_%7Bx%2Cy%7D%20%5Cwidetilde%20P(x%2Cy)%20logP(y|x)%20%2B%20constant%5C%5C
%5CRightarrow%20L_%7B%5Cwidetilde%20P%7D%20%20%3D%26%20%5Csum_%7Bx%2Cy%7D%20%5Cwidetilde%20P(x%2Cy)%20logP(y|x)
%5Cend%7Baligned%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L%3D%5Cwidetilde%20P(x)%5Csum_xlog%20P(x)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=H(X)%20%3D%20-%5Csum_xP(x)%20log%20P(x)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p%2Cq">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=D(p||q)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=D(p||q)%20%3D%20%5Csum_x%20p(x)log%20%5Cfrac%20%7Bp(x)%7D%20%7Bq(x)%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=D(p|q)%20%5Cgeqslant%200">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p%3Dq">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=D(p|q)%3D0">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D
-D(p||q)%20%3D%26%20%5Csum_x%20p(x)%20log%20%5Cfrac%20%7Bq(x)%7D%20%7Bp(x)%7D%5C%5C
(Jense)%20%5CRightarrow%20%20%20%20%20%20%20%20%20%5Cleqslant%20%26%20log%20%5Csum_xp(x)%5Cfrac%20%7Bq(x)%7D%7Bp(x)%7D%5C%5C
%20%20%20%20%20%20%20%20%20%3D%26%20log%20%5Csum_x%20q(x)%20%3D%200
%5Cend%20%7Baligned%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=E[f(x)]%20%5Cleqslant%20f(E[x])">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p(x)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Csum_xp(x)%20%3D%201">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=q">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=q">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=H(p%2Cq)%20%3D%20E_p[-log%20q]%3D%20H(p)%20%2B%20D_%7BKL%7D(p||q)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=H(p)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=D_%7BKL%7D(p||q)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=H(p%2Cq)%20%3D%20-%5Csum_x%20p(x)%20log%20q(x)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=q">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=-%5Csum_x%20%5Cwidetilde%20p(x)%20log%20p(x)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cwidetilde%20p(x)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p(x)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L_%5Cwidetilde%20p%20%3D%20%5Csum_x%20%5Cwidetilde%20p(x)log%20p(x)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=[Mark]">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p%3Dq">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p(x)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=q(x)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Csum_x%20q(x)%20%3D%201">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L(x%2C%5Clambda)%20%3D%20-%5Csum_%7Bx%7D%20p(x)%20log%20q(x)%20%2B%5Clambda%20(%5Csum_x%20p(x)%20-%201))">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Clambda">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=-%5Cfrac%20%7Bp(x)%7D%20%7Bq(x)%7D%20%2B%5Clambda%20%3D%200%5C%5C
%5Csum_x%20q(x)%20%3D%201%5C%5C
%5Csum_x%20p(x)%20%3D%201">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Clambda%20%3D%201%5C%5C
p(x)%20%3D%20q(x)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=CrossEntropy%3D−%5Csum_%7Bi%3D1%7D%20%5En%20%20y_i%5ET%20%5Ccdot%20log(h(x_i))">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y和x">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=CrossEntropy%3D-%5Csum_%7Bi%3D1%7D%5En%20(y_i%5Ccdot%20log%20(h(x_i))%20%2B(1-y_i)%20%5Ccdot%20log%20(1-(h(x_i)))">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y和x">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=h(x)">
<meta property="og:updated_time" content="2019-08-01T01:27:13.135Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="brezezee">
<meta name="twitter:description" content="[TOC] 熵、交叉熵及似然函数的关系1. 熵1.1 信息量&amp;emsp;&amp;emsp;信息量：最初的定义是信号取值数量m的对数为信息量,即 。这是与比特数相关的，比如一个信号只有两个取值，那么用1个bit便能将其表示。后来著名的香农指出，得到的信号取值结果是随机的，因此其信息量应该也是关于概率的函数，于是得到随机变量的信息量  &amp;emsp;&amp;emsp;在机器学习中，熵是描述一个变量发生的不确定性的度">
<meta name="twitter:image" content="https://www.zhihu.com/equation?tex=I">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://brezezee.github.io/MD Notes/lihang/6.logistic回归和最大熵模型/知乎上传熵、交叉熵、极大似然.html">





  <title> | brezezee</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">brezezee</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="https://brezezee.github.io" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="https://brezezee.github.io/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="https://brezezee.github.io/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-categories"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="https://brezezee.github.io/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="https://brezezee.github.io/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    
    
    
    <div class="post-block page">
      <header class="post-header">

	<h1 class="post-title" itemprop="name headline"></h1>



</header>

      
      
      
      <div class="post-body">
        
        
          <p>[TOC]</p>
<h1 id="熵、交叉熵及似然函数的关系"><a href="#熵、交叉熵及似然函数的关系" class="headerlink" title="熵、交叉熵及似然函数的关系"></a>熵、交叉熵及似然函数的关系</h1><h2 id="1-熵"><a href="#1-熵" class="headerlink" title="1. 熵"></a>1. 熵</h2><h3 id="1-1-信息量"><a href="#1-1-信息量" class="headerlink" title="1.1 信息量"></a>1.1 信息量</h3><p>&emsp;&emsp;<strong>信息量</strong>：最初的定义是信号取值数量m的对数为信息量<img src="https://www.zhihu.com/equation?tex=I" alt="I" eeimg="1">,即 <img src="https://www.zhihu.com/equation?tex=I%3Dlog_2m" alt="I=log_2m" eeimg="1">。这是与比特数相关的，比如一个信号只有两个取值，那么用1个bit便能将其表示。后来著名的香农指出，得到的信号取值结果是随机的，因此其信息量应该也是关于概率的函数，于是得到随机变量<img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1">的信息量</p>
<p align="center"><img src="https://www.zhihu.com/equation?tex=I(X%3Dx_i)%20%3D%20-logP(x_i)" alt="
I(X=x_i) = -logP(x_i)
" eeimg="1"></p>
&emsp;&emsp;在机器学习中，熵是描述一个变量发生的不确定性的度量，不确定性越大，这里面包含的信息量就越大。比如有的学生常年不及格，让你预测他这次考试是否能及格，你肯定会说十有八九不会及格，也就是能否及格的确定性很大了，这时我们不会太关心其结果，因为其发生的结果太容易预料了，所以其信息量比较小。若该学生有时候能及格，有时候不能及格，这就有点捉摸不定了，我们很难猜中他能不能及格，这时他能否及格的信息量比较大。

### 1.3 熵

&emsp;&emsp;既然熵是反应变量（信号）总体不确定性的大小（信息量大小）的度量，那么如何确定熵的大小也就很明显了——对所有可能的取值计算其平均信息量，也就是求信息量关于概率P的期望。这样就表达了该信号（随机变量）的不确定性大小。
<p align="center"><img src="https://www.zhihu.com/equation?tex=H(X)%3DE_P[-logP(x)]%3D-%5Csum_xp(x_i)logp(x_i)" alt="
H(X)=E_P[-logP(x)]=-\sum_xp(x_i)logp(x_i)
" eeimg="1"></p>
&emsp;&emsp;当X服从均匀分布时，X的熵的结果与最原始的信息量的定义相同，n为信号的取值数量
<p align="center"><img src="https://www.zhihu.com/equation?tex=H(x)%20%3D%20-%5Csum_n%20%5Cfrac%201%20n%20log%20%5Cfrac%201%20n%3D%20logn" alt="
H(x) = -\sum_n \frac 1 n log \frac 1 n= logn
" eeimg="1"></p>
实际上只有当所有事件的概率相等时，熵取最大值<img src="https://www.zhihu.com/equation?tex=logn" alt="logn" eeimg="1">。
&emsp;&emsp;于是得出我个人的猜测：信息量是描述变量取某个值时的信息量，熵则描述其所有可能取值的平均信息量，也就是该信号到底取哪个值的不确定性。

## 2. 最大熵中的极大似然函数

&emsp;&emsp;按理说熵后面应该接着说交叉熵的，但是我发现极大似然函数的对数形式与熵的定义十分相近，于是先介绍它们的来路与区别。这种形式的似然函数在最大熵模型中用到过。

&emsp;&emsp;首先我先给出在最大熵模型中用到的似然函数
<p align="center"><img src="https://www.zhihu.com/equation?tex=L_%7B%5Cwidetilde%20P%7D%20%3D%20%5Cprod_%7Bx%2Cy%7D%20P(y|x)%5E%7B%5Cwidetilde%20P(x%2Cy)%7D" alt="
L_{\widetilde P} = \prod_{x,y} P(y|x)^{\widetilde P(x,y)}
" eeimg="1"></p>
李航书中直接给出该定义，这个定义应该是由如下似然函数推导而来的
<p align="center"><img src="https://www.zhihu.com/equation?tex=L(x_1%2C%5Ccdots%2Cx_n%2C%5Ctheta)%3D%5Cprod_x%20P(x)%5E%7B%5Cwidehat%20P(x)%7D" alt="
L(x_1,\cdots,x_n,\theta)=\prod_x P(x)^{\widehat P(x)}
" eeimg="1"></p>
### 2.1 指数型似然函数推导
&emsp;&emsp;我不知道这种形式的似然函数叫什么名字，所以这个名字是我自己取的。指数型似然函数与我们常用的由n个样本带入表示的模型概率之积，也就是它们的联合概率作为似然函数不同，在常用的似然函数L中
<p align="center"><img src="https://www.zhihu.com/equation?tex=L(x%2C%5Ctheta)%3D%5Cprod%20_%7Bi%3D1%7D%5En%20P(x_i)" alt="
L(x,\theta)=\prod _{i=1}^n P(x_i)
" eeimg="1"></p>
它的概率乘积依据于样本数，最大熵则依据变量x的取值数量，然而这两种方式本质都是一样的。最大熵中的似然函数相当于把n个样本中取值相同的整理到一起，用指数数量表示。

&emsp;&emsp;假设样本集大小为<img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1">，<img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1">的取值数量为<img src="https://www.zhihu.com/equation?tex=m" alt="m" eeimg="1">，取值集合为｛<img src="https://www.zhihu.com/equation?tex=v_1%2C%5Ccdots%2Cv_m" alt="v_1,\cdots,v_m" eeimg="1">｝。样本集中观测值<img src="https://www.zhihu.com/equation?tex=v_i" alt="v_i" eeimg="1">出现的次数由<img src="https://www.zhihu.com/equation?tex=C(X%3Dv_i)" alt="C(X=v_i)" eeimg="1">表示，于是似然函数可以表示为
<p align="center"><img src="https://www.zhihu.com/equation?tex=L(x%2C%5Ctheta)%20%3D%20%5Cprod%20_%7Bi%3D1%7D%5Em%20P(x_i)%5E%7BC(X%3Dx_i)%7D" alt="
L(x,\theta) = \prod _{i=1}^m P(x_i)^{C(X=x_i)}
" eeimg="1"></p>
再对该似然函数开<img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1">次方
<p align="center"><img src="https://www.zhihu.com/equation?tex=L(x%2C%5Ctheta)%5E%7B%5Cfrac%201%20n%7D%20%3D%20%5Cprod%20_%7Bi%3D1%7D%20%5Em%20P(x_i)%20%5E%7B%5Cfrac%20%7BC(X%3Dx_i)%7D%20%7Bn%7D%7D" alt="
L(x,\theta)^{\frac 1 n} = \prod _{i=1} ^m P(x_i) ^{\frac {C(X=x_i)} {n}}
" eeimg="1"></p>
其中，
<p align="center"><img src="https://www.zhihu.com/equation?tex=%5Cfrac%20%7BC(X%3Dx_i)%7D%20%7Bn%7D%20%3D%20%5Cwidetilde%20P(x_i)" alt="
\frac {C(X=x_i)} {n} = \widetilde P(x_i)
" eeimg="1"></p>
并且，似然函数开<img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1">次方并不影响其最大化，于是可以直接将其定义为新的似然函数
<p align="center"><img src="https://www.zhihu.com/equation?tex=L(x%2C%5Ctheta)%20%20%3D%20%20%5Cprod%20_%7Bi%3D1%7D%20%5Em%20P(x_i)%20%5E%7B%5Cfrac%20%7BC(X%3Dx_i)%7D%20%7Bn%7D%7D" alt="
L(x,\theta)  =  \prod _{i=1} ^m P(x_i) ^{\frac {C(X=x_i)} {n}}
" eeimg="1"></p>
带入<img src="https://www.zhihu.com/equation?tex=%5Cwidetilde%20P(x_i)" alt="\widetilde P(x_i)" eeimg="1">简化得到
<p align="center"><img src="https://www.zhihu.com/equation?tex=L(x%2C%5Ctheta)%20%3D%20%5Cprod%20_x%20P(x)%20%5E%7B%5Cwidetilde%20P(x)%7D" alt="
L(x,\theta) = \prod _x P(x) ^{\widetilde P(x)}
" eeimg="1"></p>
那么对数似然函数便是
<p align="center"><img src="https://www.zhihu.com/equation?tex=L(x%2C%5Ctheta)%20%3D%20%5Cwidetilde%20P(x)%20%5Csum_x%20log%20P(x)" alt="
L(x,\theta) = \widetilde P(x) \sum_x log P(x) 
" eeimg="1"></p>

<h3 id="2-2-最大熵中的似然函数推导"><a href="#2-2-最大熵中的似然函数推导" class="headerlink" title="2.2 最大熵中的似然函数推导"></a>2.2 最大熵中的似然函数推导</h3><p>&emsp;&emsp;根据上文得出的似然函数的另一种表示后，由它便可推导出最大熵模型中用到的对数似然函数</p>
<p align="center"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D
L_%7B%5Cwidetilde%20P%7D%20%3D%26%20log%5Cprod_%7Bx%2Cy%7D%20P(x%2Cy)%20%5E%7B%5Cwidetilde%20P(x%2Cy)%7D%5C%5C
%3D%26%5Csum_%7Bx%2Cy%7D%20%5Cwidetilde%20P(x%2Cy)log[%5Cwidetilde%20P(x)%20P(y|x)]%5C%5C
%3D%26%5Csum_%7Bx%2Cy%7D%20%5Cwidetilde%20P(x%2Cy)logP(y|x)%20%2B%20%5Csum_%7Bx%2Cy%7D%20%5Cwidetilde%20P(x%2Cy)%20log%20%5Cwidetilde%20P(x)%5C%5C
%3D%26%20%5Csum_%7Bx%2Cy%7D%20%5Cwidetilde%20P(x%2Cy)%20logP(y|x)%20%2B%20constant%5C%5C
%5CRightarrow%20L_%7B%5Cwidetilde%20P%7D%20%20%3D%26%20%5Csum_%7Bx%2Cy%7D%20%5Cwidetilde%20P(x%2Cy)%20logP(y|x)
%5Cend%7Baligned%7D" alt="
\begin{aligned}
L_{\widetilde P} =& log\prod_{x,y} P(x,y) ^{\widetilde P(x,y)}\\
=&\sum_{x,y} \widetilde P(x,y)log[\widetilde P(x) P(y|x)]\\
=&\sum_{x,y} \widetilde P(x,y)logP(y|x) + \sum_{x,y} \widetilde P(x,y) log \widetilde P(x)\\
=& \sum_{x,y} \widetilde P(x,y) logP(y|x) + constant\\
\Rightarrow L_{\widetilde P}  =& \sum_{x,y} \widetilde P(x,y) logP(y|x)
\end{aligned}
" eeimg="1"></p>
推导这个只是顺便，目的还是比较指数型对数似然函数<img src="https://www.zhihu.com/equation?tex=L%3D%5Cwidetilde%20P(x)%5Csum_xlog%20P(x)" alt="L=\widetilde P(x)\sum_xlog P(x)" eeimg="1">与熵的定义的关系。

&emsp;&emsp;再回顾熵的定义
<p align="center"><img src="https://www.zhihu.com/equation?tex=H(X)%20%3D%20-%5Csum_xP(x)%20log%20P(x)" alt="
H(X) = -\sum_xP(x) log P(x)
" eeimg="1"></p>
它们的数学形式长得确实挺像的，不过仔细看还是有很大差别，似然函数中具有经验分布和需要估计的模型的分布，所以似然函数实际上是判断估计模型与经验分布的**相似度**，最大似然估计就是让估计模型尽可能的去接近我们得到样本产生的经验分布。而熵的定义中只有概率模型本身，它表示的是该模型本身的每个取值之间的不确定性大小，或者说是该模型本身的混乱程度。而最大熵原理的思想是在面对一个未知内容较多（约束条件不足）的估计时，我们会尽量将其看做等概率的均匀分布。

&emsp;&emsp;由最大熵原理推导出的最大熵模型的思想就是在满足假设条件的模型中，选择模型内部最混乱，也就是信息量最大的那个模型！

## 3. 交叉熵与极大似然

&emsp;&emsp;上述解释了熵与极大似然的一个重要区别在于熵是描述模型本身混乱度（信息量）的一个度量，而极大似然是描述估计模型和经验分布的相似度的一个度量。那么熵到底与指数型似然函数有什么关系呢？

### 3.1 联系

&emsp;&emsp;熵描述模型内部的信息量，而交叉熵则描述两个模型之间的关系。交叉熵中用到了KL散度，KL散度是描述两个概率分布<img src="https://www.zhihu.com/equation?tex=p%2Cq" alt="p,q" eeimg="1">相似性的一种度量，记作<img src="https://www.zhihu.com/equation?tex=D(p||q)" alt="D(p||q)" eeimg="1">。对于离散随机变量，KL散度定义为
<p align="center"><img src="https://www.zhihu.com/equation?tex=D(p||q)%20%3D%20%5Csum_x%20p(x)log%20%5Cfrac%20%7Bp(x)%7D%20%7Bq(x)%7D" alt="
D(p||q) = \sum_x p(x)log \frac {p(x)} {q(x)}
" eeimg="1"></p>
&emsp;&emsp;KL散度满足：<img src="https://www.zhihu.com/equation?tex=D(p|q)%20%5Cgeqslant%200" alt="D(p|q) \geqslant 0" eeimg="1">，且仅当<img src="https://www.zhihu.com/equation?tex=p%3Dq" alt="p=q" eeimg="1">时，<img src="https://www.zhihu.com/equation?tex=D(p|q)%3D0" alt="D(p|q)=0" eeimg="1">。证明如下：
<p align="center"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D
-D(p||q)%20%3D%26%20%5Csum_x%20p(x)%20log%20%5Cfrac%20%7Bq(x)%7D%20%7Bp(x)%7D%5C%5C
(Jense)%20%5CRightarrow%20%20%20%20%20%20%20%20%20%5Cleqslant%20%26%20log%20%5Csum_xp(x)%5Cfrac%20%7Bq(x)%7D%7Bp(x)%7D%5C%5C
%20%20%20%20%20%20%20%20%20%3D%26%20log%20%5Csum_x%20q(x)%20%3D%200
%5Cend%20%7Baligned%7D" alt="
\begin{aligned}
-D(p||q) =& \sum_x p(x) log \frac {q(x)} {p(x)}\\
(Jense) \Rightarrow         \leqslant & log \sum_xp(x)\frac {q(x)}{p(x)}\\
         =& log \sum_x q(x) = 0
\end {aligned}
" eeimg="1"></p>
&emsp;&emsp;关于Jense不等式在EM算法中已经总结过了，这里不再赘述。这里log是凹函数，<img src="https://www.zhihu.com/equation?tex=E[f(x)]%20%5Cleqslant%20f(E[x])" alt="E[f(x)] \leqslant f(E[x])" eeimg="1">。注意<img src="https://www.zhihu.com/equation?tex=p(x)" alt="p(x)" eeimg="1">满足<img src="https://www.zhihu.com/equation?tex=%5Csum_xp(x)%20%3D%201" alt="\sum_xp(x) = 1" eeimg="1">。由定义式可以看出KL散度是非对称的，不是严格意义上的距离度量。

&emsp;&emsp;Wiki对交叉熵的定义是：在信息论中，基于相同事件测度的两个概率分布<img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1">和 <img src="https://www.zhihu.com/equation?tex=q" alt="q" eeimg="1">的交叉熵是指，当基于一个“非自然”（相对于“真实”分布<img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1">而言）的概率分布 <img src="https://www.zhihu.com/equation?tex=q" alt="q" eeimg="1">进行编码时，在事件集合中唯一标识一个事件所需要的平均比特数（bit）。基于概率的交叉熵定义为
<p align="center"><img src="https://www.zhihu.com/equation?tex=H(p%2Cq)%20%3D%20E_p[-log%20q]%3D%20H(p)%20%2B%20D_%7BKL%7D(p||q)" alt="
H(p,q) = E_p[-log q]= H(p) + D_{KL}(p||q)
" eeimg="1"></p>
其中，<img src="https://www.zhihu.com/equation?tex=H(p)" alt="H(p)" eeimg="1">是熵，<img src="https://www.zhihu.com/equation?tex=D_%7BKL%7D(p||q)" alt="D_{KL}(p||q)" eeimg="1">是p到q的KL散度，对于离散的p，q
<p align="center"><img src="https://www.zhihu.com/equation?tex=H(p%2Cq)%20%3D%20-%5Csum_x%20p(x)%20log%20q(x)" alt="
H(p,q) = -\sum_x p(x) log q(x)
" eeimg="1"></p>
&emsp;&emsp;这个定义与最开始的信息量和熵的定义是同源的，关于比特数表示可以参照信息量的定义。现在讨论关于p和q的关系，Wiki中说<img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1">是一个“真实”分布，在此基础上对<img src="https://www.zhihu.com/equation?tex=q" alt="q" eeimg="1">进行二进制编码所需要的平均比特数。如果对这个说法不能很好理解的话，可以将其转化为下面这个我们经常见到的形式
<p align="center"><img src="https://www.zhihu.com/equation?tex=-%5Csum_x%20%5Cwidetilde%20p(x)%20log%20p(x)" alt="
-\sum_x \widetilde p(x) log p(x)
" eeimg="1"></p>
其中，<img src="https://www.zhihu.com/equation?tex=%5Cwidetilde%20p(x)" alt="\widetilde p(x)" eeimg="1">是经验分布，看做我们从样本中得到的“真实”分布，而<img src="https://www.zhihu.com/equation?tex=p(x)" alt="p(x)" eeimg="1">是我们需要编码（求解）的模型分布。

&emsp;&emsp;再来看看上面提到的指数型对数似然函数
<p align="center"><img src="https://www.zhihu.com/equation?tex=L_%5Cwidetilde%20p%20%3D%20%5Csum_x%20%5Cwidetilde%20p(x)log%20p(x)" alt="
L_\widetilde p = \sum_x \widetilde p(x)log p(x) 
" eeimg="1"></p>
它们正好是相反数，因此极大化似然函数等价于最小化交叉熵。<img src="https://www.zhihu.com/equation?tex=[Mark]" alt="[Mark]" eeimg="1">：这个结论在大多数时候成立，但由于刚学不久，很多知识还不知道，所以目前也不能给出明确说法及证明。网上有说满足广义伯努利分布的便等价的，也有说都等价的，暂且先从直观上理解吧。

&emsp;&emsp;根据定义，交叉熵的含义也比较明显了，它也是表示两个概率模型之间的相似度的。这也与极大似然函数的含义相同。若两个概率模型完全相同呢？由KL散度可知，<img src="https://www.zhihu.com/equation?tex=p%3Dq" alt="p=q" eeimg="1">时散度为0，交叉熵变成了熵。从离散形式的定义式也能看出，相等时变为了熵的定义式，交叉熵退化为熵。并且此时交叉熵取最小值，证明如下：

&emsp;&emsp;假设<img src="https://www.zhihu.com/equation?tex=p(x)" alt="p(x)" eeimg="1">分布已知，其值为常数，且<img src="https://www.zhihu.com/equation?tex=q(x)" alt="q(x)" eeimg="1">满足约束条件
<p align="center"><img src="https://www.zhihu.com/equation?tex=%5Csum_x%20q(x)%20%3D%201" alt="
\sum_x q(x) = 1
" eeimg="1"></p>
构造拉格朗日乘子函数
<p align="center"><img src="https://www.zhihu.com/equation?tex=L(x%2C%5Clambda)%20%3D%20-%5Csum_%7Bx%7D%20p(x)%20log%20q(x)%20%2B%5Clambda%20(%5Csum_x%20p(x)%20-%201))" alt="
L(x,\lambda) = -\sum_{x} p(x) log q(x) +\lambda (\sum_x p(x) - 1))
" eeimg="1"></p>
对<img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1">和所有的<img src="https://www.zhihu.com/equation?tex=x" alt="x" eeimg="1">求偏导得
<p align="center"><img src="https://www.zhihu.com/equation?tex=-%5Cfrac%20%7Bp(x)%7D%20%7Bq(x)%7D%20%2B%5Clambda%20%3D%200%5C%5C
%5Csum_x%20q(x)%20%3D%201%5C%5C
%5Csum_x%20p(x)%20%3D%201" alt="
-\frac {p(x)} {q(x)} +\lambda = 0\\
\sum_x q(x) = 1\\
\sum_x p(x) = 1
" eeimg="1"></p>
注意，第一个式子实际上有m个等式，m是变量x的取值个数，解得
<p align="center"><img src="https://www.zhihu.com/equation?tex=%5Clambda%20%3D%201%5C%5C
p(x)%20%3D%20q(x)" alt="
\lambda = 1\\
p(x) = q(x)
" eeimg="1"></p>

<h3 id="3-2-交叉熵损失函数"><a href="#3-2-交叉熵损失函数" class="headerlink" title="3.2 交叉熵损失函数"></a>3.2 交叉熵损失函数</h3><p>&emsp;&emsp;至此，再联系一下交叉熵损失函数。交叉熵损失函数经常用于分类问题中，特别是在神经网络做分类问题时，也经常使用交叉熵作为损失函数，此外，由于交叉熵涉及到计算每个类别的概率，所以交叉熵几乎每次都和<strong>softmax函数</strong>一起出现。</p>
<p>&emsp;&emsp;交叉熵损失函数的一般形式是</p>
<p></p><p align="center"><img src="https://www.zhihu.com/equation?tex=CrossEntropy%3D−%5Csum_%7Bi%3D1%7D%20%5En%20%20y_i%5ET%20%5Ccdot%20log(h(x_i))" alt="
CrossEntropy=−\sum_{i=1} ^n  y_i^T \cdot log(h(x_i))
" eeimg="1"></p><br>其中，<img src="https://www.zhihu.com/equation?tex=y和x" alt="y和x" eeimg="1">是m维列向量，m是<img src="https://www.zhihu.com/equation?tex=y" alt="y" eeimg="1">的取值数量。当<img src="https://www.zhihu.com/equation?tex=y" alt="y" eeimg="1">取值为｛0,1｝时，就是我们常见的0-1分布的交叉熵损失函数<p></p>
<p></p><p align="center"><img src="https://www.zhihu.com/equation?tex=CrossEntropy%3D-%5Csum_%7Bi%3D1%7D%5En%20(y_i%5Ccdot%20log%20(h(x_i))%20%2B(1-y_i)%20%5Ccdot%20log%20(1-(h(x_i)))" alt="
CrossEntropy=-\sum_{i=1}^n (y_i\cdot log (h(x_i)) +(1-y_i) \cdot log (1-(h(x_i)))
" eeimg="1"></p><br>这里<img src="https://www.zhihu.com/equation?tex=y和x" alt="y和x" eeimg="1">是数值。<p></p>
<p>&emsp;&emsp;交叉熵损失函数与交叉熵的区别在于将原来的“真实”概率分布替换为“真实”标签（label）<img src="https://www.zhihu.com/equation?tex=y_i" alt="y_i" eeimg="1">，这与前面推导指数型似然函数的原理基本是一致的。不再赘述。将公式中<img src="https://www.zhihu.com/equation?tex=h(x)" alt="h(x)" eeimg="1">替换为logistic中的分布函数则得到logistic中的损失函数。</p>
<h2 id="Reference："><a href="#Reference：" class="headerlink" title="Reference："></a>Reference：</h2><p><a href="https://book.douban.com/subject/10590856/" target="_blank" rel="noopener">统计学习方法</a></p>
<p><a href="https://book.douban.com/subject/2061116/" target="_blank" rel="noopener">Pattern Recognition and Machine Learning</a></p>

        
      </div>
      
      
      
    </div>
    
    
    
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">brezezee</p>
              <p class="site-description motion-element" itemprop="description">靡不有初，鲜克有终</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="https://brezezee.github.io/archives">
              
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="../../../categories/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="../../../tags/index.html">
                  <span class="site-state-item-count">34</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#熵、交叉熵及似然函数的关系"><span class="nav-text">熵、交叉熵及似然函数的关系</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-熵"><span class="nav-text">1. 熵</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-信息量"><span class="nav-text">1.1 信息量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-最大熵中的似然函数推导"><span class="nav-text">2.2 最大熵中的似然函数推导</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-交叉熵损失函数"><span class="nav-text">3.2 交叉熵损失函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference："><span class="nav-text">Reference：</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">brezezee</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">59.1k</span>
  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="../../../lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="../../../lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="../../../lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="../../../lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="../../../lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="../../../lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="../../../lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="../../../js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="../../../js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="../../../js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="../../../js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="../../../js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'https://brezezee.github.io/MD Notes/lihang/6.logistic回归和最大熵模型/知乎上传熵、交叉熵、极大似然.html';
          this.page.identifier = 'MD Notes/lihang/6.logistic回归和最大熵模型/知乎上传熵、交叉熵、极大似然.html';
          this.page.title = '';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
